{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering #\n",
    "\n",
    "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we're going to use k-means clustering in order to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup # remember this one! \n",
    "import re # this too! \n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Corpus Pre-Proccesing ##\n",
    "\n",
    "Here we go again. Let's pre-process our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import three lists: titles, links, and wikipedia synopses\n",
    "titles = open('title_list.txt').read().split('\\n')\n",
    "\n",
    "# ensures that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "links = open('link_list_imdb.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    # strips html formatting and converts to unicode\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "    \n",
    "genres = open('genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(links)) + ' links')\n",
    "print(str(len(synopses_wiki)) + ' synopses')\n",
    "print(str(len(genres)) + ' genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get the imdb synopses\n",
    "\n",
    "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    #strips html formatting and converts to unicode\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the two sets of synopses\n",
    "\n",
    "synopses = []\n",
    "\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "\n",
    "# see what one looks like\n",
    "print(synopses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For document clustering, some people like to stem first ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob # an alternative to spaCy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TF-IDF (once again!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, \n",
    "                                   tokenizer=textblob_tokenizer, ngram_range=(1,3),\n",
    "                                   min_df=.2, max_df=0.8) #note new params\n",
    "\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print(tfidf_vectorizer_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our feature names for future reference \n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first synopsis) to see what it looks like\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
    "\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: On to the K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that K-means initializes with a pre-determined number of clusters. Let's choose 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know \n",
    "\n",
    "km.fit(tfidf_vectorizer_vectors)\n",
    "\n",
    "# km.labels_ gives you the cluster assignments\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump our clusters into a dataframe\n",
    "films = { 'title': titles, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "film_df = pd.DataFrame(films, columns = ['title', 'cluster', 'genre'])\n",
    "\n",
    "film_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many films are in each cluster\n",
    "film_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top terms per cluster\n",
    "\n",
    "# this orders by the distance of each term from the center\n",
    "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster \" + str(i) + \" top words: \")\n",
    "    top_terms = \"\"\n",
    "   \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        top_terms += terms[ind] + \", \"\n",
    "  \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top films per cluster    \n",
    "for i in range(num_clusters):  \n",
    "    print(\"Titles in cluster \" + str(i) + \": \")\n",
    "    cluster_titles = \"\"\n",
    "\n",
    "    # create new df of only the specific cluster\n",
    "    # remember boolean selection! \n",
    "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
    " \n",
    "    # create series of titles assoc w/ that cluster \n",
    "    for title in cluster_df['title']: \n",
    "        cluster_titles += title + \", \"\n",
    "\n",
    "    print(cluster_titles + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing Document Clusters\n",
    "\n",
    "### Part 1: Dimensionality Reduction with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "predictions = km.fit_predict(tfidf_vectorizer_vectors.toarray())\n",
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tsne.fit_transform(tfidf_vectorizer_vectors.toarray())\n",
    "xs, ys = zip(*embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Actually visualizing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Cluster 0', \n",
    "                 1: 'Cluster 1', \n",
    "                 2: 'Cluster 2', \n",
    "                 3: 'Cluster 3', \n",
    "                 4: 'Cluster 4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the t-sne plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "    \n",
    "plt.show() #show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another clustering method: hierarchical document clustering\n",
    "\n",
    "Here's an example of hierarchical document clustering using Ward's method, which relies on minimum variance. [more here](https://www.statisticshowto.datasciencecentral.com/wards-method/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"left\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
