{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling ##\n",
    "\n",
    "*This lesson draws on blog posts by [Ted Underwood](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) and [Matthew Jockers](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/), this [video of a talk by David Mimno](https://vimeo.com/53080123), and [this notebook](https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html) by Radim Rehurek. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging # for logging status etc\n",
    "import itertools # helpful library for iterating through things\n",
    "\n",
    "import numpy as np # this is a powerful python math package that many others are based on\n",
    "import gensim # our topic modeling library\n",
    "import os # for file i/o\n",
    "\n",
    "# configure logging \n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  \n",
    "\n",
    "# a helpful function that returns the first `n` elements of the stream as plain list.\n",
    "# we'll use this later\n",
    "def head(stream, n=10):\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some more modules for processing the corpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already, please download and unzip the Colored Conventions Corpus: [link](http://coloredconventions.org/intro-corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing ###\n",
    "\n",
    "Many NLP tasks require that you first tokenize your corpus. We actually already tokenized something when we chunked our song lyrics by line. \n",
    "\n",
    "Here is [another example of tokenizing](https://programminghistorian.org/en/lessons/sentiment-analysis) that uses nltk to tokenize a document by sentence instead. (Scroll down to where it discusses the nltk word_tokenize module). **ATTENTION! This may be helpful to you for your next homework!**\n",
    "\n",
    "Here, however, we're going to write our own quick tokenizing function that makes use of gensim's [simple_preprocess function](https://radimrehurek.com/gensim/utils.html), which breaks a document into a list of lowercase tokens. The lower-casing is important for topic modeling since we want both uppercase and lowercase versions of the same word to be counted together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's some nice dense python for you\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further pre-processing our corpus ###\n",
    "\n",
    "This is the other necessary step before running a topic model. You need to write a function that iterates through your corpus and returns each document in the format (title, tokens). \n",
    "\n",
    "**Side note that we've not yet discussed tuples.** Tuples exist in many programming languages, including R, I think. If not, or even if so, just know that tuples are sequences of objects--  just like lists-- but they cannot be changed. In Python, you indicate a tuple with parentheses. \n",
    "\n",
    "In any case, we will want a pre-processing function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to yield each doc in CCP Corpus as a `(filename, tokens)` tuple.\n",
    "\n",
    "def iter_docs(base_dir):\n",
    "    docCount = 0\n",
    "    docs = os.listdir(base_dir)\n",
    "\n",
    "    for doc in docs:\n",
    "        if not doc.startswith('.'):\n",
    "            with open(base_dir + doc, \"r\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text) \n",
    "        \n",
    "                yield doc, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the stream for later processing \n",
    "stream = iter_docs('./2019-09-ccp-corpus-0.3/ccprecords/')\n",
    "\n",
    "# while we're at it, take a look at what this looks like for the first five docs\n",
    "for doc, tokens in itertools.islice(stream, 5):\n",
    "    print(doc, tokens[:10])  # print the doc title and its first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a Dictionary (not to be confused with a Python dictionary) which maps each word to a numerical ID. \n",
    "\n",
    "This mapping step is required because most algorithms, including gensim's implementation of LDA, rely on numerical libraries that work with vectors indexed by integers, not by strings. Also, many need to know the vector/matrix dimensionality in advance.\n",
    "\n",
    "The mapping can be constructed automatically by giving gensim's Dictionary class a stream of tokenized documents, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the CCP Corpus Dictionary\n",
    "\n",
    "doc_stream = (tokens for _, tokens in iter_docs('./2019-09-ccp-corpus-0.3/ccprecords/'))\n",
    "              \n",
    "id2word_ccp = gensim.corpora.Dictionary(doc_stream) \n",
    "\n",
    "print(id2word_ccp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dictionary (id2word_ccp) now contains all words that appeared in the corpus, along with how many times they appeared. \n",
    "\n",
    "gensim provides a handy function for mapping tokens to their ID numbers, viz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2word_ccp.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many things you need to do in order to tune your topic model, but one important thing do consider is whether you should filter the words. \n",
    "\n",
    "gensim also provides functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out 50 most frequent words\n",
    "# id2word_ccp.filter_n_most_frequent(50)\n",
    "\n",
    "# filter out words in only 1 doc, keeping the rest\n",
    "# note how no_below and no_above take different values\n",
    "id2word_ccp.filter_extremes(no_below=2, no_above=1.0)\n",
    "\n",
    "print(id2word_ccp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by removing the words that only appeared in a single document, we went from 23,844 unique words (or tokens) to 14,014. That's not a huge number for a topic model, and as you'll see, there are probably other methods that would work better for this corpus. We'll explore some of those next class. \n",
    "\n",
    "But for now, since a streamed corpus and a dictionary is all we need to create the vectors for our topic model, we can get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class we need; this is the same for every topic model you create with gensim\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream of bag-of-words vectors\n",
    "ccp_corpus = Corpus('./2019-09-ccp-corpus-0.3/ccprecords/', id2word_ccp)\n",
    "\n",
    "# print the first vector in the stream to see what it looks like; \n",
    "# this is in the format (word_id, count in first doc)\n",
    "\n",
    "vector = next(iter(ccp_corpus))\n",
    "print(vector)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're ready to run our topic model!\n",
    "\n",
    "%time lda_model = gensim.models.LdaModel(ccp_corpus, num_topics=15, id2word=id2word_ccp, passes=10) \n",
    "\n",
    "# note that passes should be higher -- people usually do 50-100 -- \n",
    "# but in the interests of time we'll only do 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some additional helpful functions built into LdaModel\n",
    "\n",
    "# how to store corpus to disk\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('./ccp.corpus.mm', ccp_corpus)\n",
    "\n",
    "# how to store dictionary to disk\n",
    "id2word_ccp.save('./ccp.dictionary')\n",
    "\n",
    "# how to store model to disk \n",
    "lda_model.save('./lda_ccp-15topics_10iters.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an old model; in this case, a topic model of the ccp with 50 iterations\n",
    "lda_model = gensim.models.LdaModel.load('./lda_ccp-15topics_50iters.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 20 terms in each topic\n",
    "\n",
    "lda_model.show_topics(15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's format the words nicely; \n",
    "# the formatted=False parameter returns tuples of (word, probability)\n",
    "\n",
    "topics = lda_model.show_topics(15, 20, formatted=False)\n",
    "\n",
    "for topic in topics:\n",
    "    topic_num = topic[0]\n",
    "    topic_words = \"\"\n",
    "    \n",
    "    topic_pairs = topic[1]\n",
    "    for pair in topic_pairs:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(\"T\" + str(topic_num) + \": \" + topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a bit of a closer look at the probabilities attached to each word \n",
    "# in a single topic \n",
    "\n",
    "# T0 looks decent\n",
    "topic = topics[0]\n",
    "\n",
    "# this is the topic number\n",
    "topic_num = topic[0]\n",
    "\n",
    "topic_pairs = topic[1]\n",
    "for pair in topic_pairs:\n",
    "    print(pair[0] + \": \" + str(pair[1]))\n",
    "\n",
    "# since all topics contain all words, the sum of all of the probabilities of each \n",
    "# topic should be 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flip it around and look at the document composition \n",
    "# Mallet does this automatically, but with gensim's built-in topic modeling \n",
    "# algorithm, we need to do it manually\n",
    "\n",
    "tokens = [] \n",
    "\n",
    "# open one file\n",
    "with open('./2019-09-ccp-corpus-0.3/ccprecords/1843.NY-08.15.BUFF.MIN.01.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "    tokens = tokenize(text) # remember this from above\n",
    "\n",
    "# create the bag of words for the document on the basis of the CCP dictionary, created above\n",
    "doc_bow = id2word_ccp.doc2bow(tokens)\n",
    "\n",
    "# get the topics that the doc consists of\n",
    "doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "doc_topics\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can cross-reference to find those topics and words\n",
    "\n",
    "for topic, prob in doc_topics:\n",
    "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document.\")\n",
    "          \n",
    "        #  str(round(prob, 2)))\n",
    "\n",
    "    topic_words = \"Top words in topic: \"\n",
    "    select_topics = topics[topic]\n",
    "    \n",
    "    for pair in select_topics[1]:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(topic_words)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Topics ###\n",
    "\n",
    "Gensim has several built-in methods for evaluating topics, including something called [topic coherence](https://rare-technologies.com/what-is-topic-coherence/). \n",
    "\n",
    "While it's time-consuming, one way to determine whether you've selected the appropriate number of topics is to calculate the coherence score for different numbers of topics. The higher the score, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=lda_model, corpus=ccp_corpus, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "\n",
    "coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But another way to evalute topics is to look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA visualization tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "# just reformat the corpus for pyLDAvis \n",
    "ccp_mm_corpus = MmCorpus('./ccp.corpus.mm')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda_model, ccp_mm_corpus, id2word_ccp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
