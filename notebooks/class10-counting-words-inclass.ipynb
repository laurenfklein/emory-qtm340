{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words, Part 1: TF/IDF ##\n",
    "\n",
    "*Based of tutorials by [Matthew Lavin](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) and [Kavita Ganesan](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XZVlcOdKhSw)*\n",
    "\n",
    "Topic modelling can be a powerful technique for identifying the themes that recur in a corpus. But in many cases, just counting words can also tell you a lot. \n",
    "\n",
    "To begin, we're going to explore a method called Term Frequency - Inverse Document Frequency (tf-idf). Tf-idf comes up a lot in text analysis projects because it’s both a corpus exploration method and a pre-processing step for many other text-mining measures and models.\n",
    "\n",
    "The procedure was introduced in a 1972 paper by Karen Spärck Jones under the name “term specificity,” and the basic idea is this:\n",
    "\n",
    "Instead of representing a term in a document by its raw frequency (its number of occurrences) or its relative frequency (the term count divided by the document length), each term is *weighted* by dividing the term frequency by the number of documents in the corpus containing the word. \n",
    "\n",
    "The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in a document are often the most frequently used words in all of the documents. We encountered this very problem in our topic model of the CCP Corpus.\n",
    "\n",
    "By contrast, terms with the highest tf-idf scores are the terms in a document that are distinctively frequent in a document, when that document is compared other documents. When you sort by tf-idf score, these distinctive terms rise to the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: How to do it ###\n",
    "\n",
    "To start, create a list in which each doc is a single string. Note that this is nearly the same as our iter_docs function from last class. You'll get very familiar with writing document and text pre-processing code like this by the end of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"./2019-09-ccp-corpus-0.3/ccprecords/\"\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "for doc in docs:\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\") as file:\n",
    "            text = file.read()\n",
    "            all_docs.append(text)\n",
    "\n",
    "# just take a look at the first item to be sure\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can  calculate tf-idf manually, but we'll use scikit-learn's tf-idf modules because they have built-in tokenization. \n",
    "\n",
    "Note also that we're encountering a *third* library that does tokenization for us. The takeaway is that after getting your text into whatever format is required, the next step us (usually) to tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(all_docs)\n",
    "\n",
    "# check shape\n",
    "word_count_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the whole vocabulary and counts like this\n",
    "cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can sort it like this:\n",
    "\n",
    "sum_words = word_count_vector.sum(axis=0) # sum_words is a vector that contains\n",
    "                                            # the sum of each word occurrence in all \n",
    "                                            # texts in the corpus. In other words, \n",
    "                                            # we are adding the elements for each column of\n",
    "                                            # the word_count_vector matrix\n",
    "\n",
    "# then sort the list of tuples that contain the word and their occurrence in the corpus.\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display the top 10\n",
    "words_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw word frequencies: not that interesting! \n",
    "\n",
    "So now let's calculate the IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # this will help us keep track of our data; \n",
    "# we'll talk about pandas and dataframes in more detail on Tuedsay\n",
    "\n",
    "# Call tfidf_transformer.fit on the word count vector we computed earlier.\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what are these weights more precisely?\n",
    "\n",
    "The most direct formula would be **N/df<sub>i</sub>**, where N represents the total number of documents in the corpus, and df is the number of documents in which the term appears. \n",
    "\n",
    "However, many implementations of tf-idf, including scikit-learn, normalize the results with additional operations. \n",
    "\n",
    "In tf-idf, normalization is generally used in two ways, and for two reasons: first, to prevent bias in term frequency from terms in shorter or longer documents; and second, as above, to calculate each term’s idf value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn’s implementation of tf-idf represents N as **N+1**, calculates the natural logarithm of **(N+1)/df<sub>i</sub>**, and then adds **1** to the final result. \n",
    "\n",
    "![tf-idf](http://lklein.lmc.gatech.edu/wp-content/uploads/2019/10/Screen-Shot-2019-10-02-at-11.52.31-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, one you have the IDF values, you can now compute the tf-idf scores for any document or set of documents. Let’s compute tf-idf scores for the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s print the tf-idf values of the first document to see if it makes sense. What we are doing below is, placing the tf-idf scores from the first document into a pandas data frame and sorting it in descending order of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores for the first doc\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only certain words have scores. This is because all the words in this document have a tf-idf score and everything else shows up as zeroes.\n",
    "\n",
    "Also, there are still some very common words that are evidently distinctive, but they're also just not that interesting. \n",
    "\n",
    "So now we're going to do it again with scikit-learn's stopword list. And since we're tf-idf pros, we're going to use scikit-learn's all-in-one TF-IDF vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's store our TF-IDF vectors to files so that we can use them next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./2019-09-ccp-corpus-0.3/ccprecords/\"\n",
    "\n",
    "# make a directory to store them in\n",
    "os.mkdir(\"./tf_idf_output\")\n",
    "\n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "csvs = []\n",
    "\n",
    "for doc in docs:\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        csv = doc.replace(\".txt\",\".csv\")\n",
    "        csvs.append(csv)\n",
    "\n",
    "# convert sparse matrix to array\n",
    "tfidf_vectors_as_array = tfidf_vectorizer_vectors.toarray()\n",
    "\n",
    "# loop each item in tfidf_vectors_as_array, \n",
    "for counter, doc in enumerate(tfidf_vectors_as_array): # note enumerate. useful! \n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(tfidf_vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(csvs[counter])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
