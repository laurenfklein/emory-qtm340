{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering #\n",
    "\n",
    "*Based on materials by Brandon Rose, Jacob Eisenstein, and Eun Seo Jo et al.*\n",
    "\n",
    "Last class, we learned about document classification, which assumes the following setup:\n",
    "\n",
    "• a training set where you get observations x and labels y;\n",
    "• a test set where you only get observations x.\n",
    "\n",
    "But what happens when you don't have labeled data? Is it possible to learn anything? \n",
    "\n",
    "This scenario is known as *unsupervised learning,* and we will see that it is very possible to learn about the underlying structure of unlabeled observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning: a quick first example ###\n",
    "\n",
    "One place where unsupervised learning is used is in word sense disambiguation. Here, the goal is to classify each instance of a word, such as bank, into one of these two senses:\n",
    "\n",
    "• `bank#1`: a financial institution\n",
    "\n",
    "• `bank#2`: the land bordering a river\n",
    "\n",
    "It is difficult to obtain sufficient training data for word sense disambiguation, because even a large corpus will contain only a few instances of all but the most common words. \n",
    "\n",
    "Is it possible to learn anything about these different senses without labeled data?\n",
    "\n",
    "Word sense disambiguation is usually performed using feature vectors constructed from the local context of the word to be disambiguated. For example, for the word `bank`, the immediate context might typically include words from one of the following two groups:\n",
    "\n",
    "1. financial, deposits, credit, lending, capital, markets, regulated, reserve, liquid, assets \n",
    "\n",
    "2. land, water, geography, stream, river, flow, deposits, discharge, channel, ecology\n",
    "\n",
    "Now consider a scatterplot, in which each point is a document containing the word bank: \n",
    "\n",
    "![banks](http://lklein.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-30-at-3.06.18-PM.png)\n",
    "\n",
    "*Image from Jacob Eistenstein, Introduction to Natural Language Processing (MIT Press, 2020)*\n",
    "\n",
    "Here, the location of the document on the x-axis is the count of words in group 1, and the location on the y-axis is the count for group 2. In such a plot, two “blobs” might emerge, and these blobs correspond to the different senses of bank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second example: document clustering ###\n",
    "\n",
    "Here’s a scenario more related to our class: \n",
    "\n",
    "Suppose you download thousands of news articles, and then make a scatterplot as above. In this hypothetical scatter plot, each point corresponds to a document rather than a word. The x-axis is the frequency of one group of words: hurricane, winds, storm; the y-axis is the frequency of another group of words: election, voters, vote. This time, three blobs might emerge: one for documents that are largely about a hurricane, another for documents largely about a election, and a third for documents about neither topic.\n",
    "\n",
    "These clumps represent the underlying structure of the data. But the two-dimensional scatter plots we have just discussed are based on groupings of context words, whereas in real scenarios, these word lists are unknown. \n",
    "\n",
    "Unsupervised learning applies the same basic idea, but in a high-dimensional space with one dimension for every context word. As with the other high-dimensional models we've explored in this course, the space can’t be directly visualized. But the goal is the same: try to identify the underlying structure of the observed data, such that there are a few clusters of points, each of which is internally coherent. \n",
    "\n",
    "**Clustering algorithms** are capable of finding such structure automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter K-Means Clustering ##\n",
    "\n",
    "Clustering algorithms assign each data point to a discrete cluster, zi ∈ 1, 2, . . . K . One of the best known clustering algorithms is K-means, an iterative algorithm that maintains a cluster assignmetn for each instance, and a central (\"mean\") location for each cluster. K-means iterates between updates to the assignments and the centers:\n",
    "\n",
    "1. each instance is placed in the cluster with the closest center;\n",
    "\n",
    "2. each center is recomputed as the average over points in the cluster.\n",
    "\n",
    "If you're curious, here is the algorithm that formalizes it:\n",
    "\n",
    "![k-means](http://lklein.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-30-at-3.27.48-PM.png)\n",
    "\n",
    "*Image from Jacob Eistenstein, Introduction to Natural Language Processing (MIT Press, 2020)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** An important property of K-means is that the converged solution depends on the initialization, and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to it! ##\n",
    "\n",
    "In this example, we're going to use k-means clustering in order to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list), a corpus created by Brandon Rose. See [the original post](http://www.brandonrose.org/top100) for a more detailed discussion on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup # remember this one! \n",
    "import re # this too! \n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Pre-Proccesing ##\n",
    "\n",
    "Here we go again. Let's pre-process our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import three lists: titles, links, and wikipedia synopses\n",
    "titles = open('title_list.txt').read().split('\\n')\n",
    "\n",
    "# ensures that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "links = open('link_list_imdb.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    # strips html formatting and converts to unicode\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "    \n",
    "genres = open('genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(links)) + ' links')\n",
    "print(str(len(synopses_wiki)) + ' synopses')\n",
    "print(str(len(genres)) + ' genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get the imdb synopses\n",
    "\n",
    "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    #strips html formatting and converts to unicode\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the two sets of synopses\n",
    "\n",
    "synopses = []\n",
    "\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "\n",
    "# see what one looks like\n",
    "print(synopses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For document clustering, some people like to stem first ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install textblob # an alternative to spaCy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now TF-IDF (again!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, \n",
    "                                   tokenizer=textblob_tokenizer, ngram_range=(1,3),\n",
    "                                   min_df=.2, max_df=0.8) #note new params\n",
    "\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print(tfidf_vectorizer_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our feature names for future reference \n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first synopsis) just to see what it looks like (we have done this before)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=terms, columns=[\"tfidf\"])\n",
    "\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And on to the K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our tf-idf vectors, we can now run the k-means clustering algorithm. Remember that K-means initializes with a pre-determined number of clusters. Let's choose 5. \n",
    "\n",
    "**NOTE:** In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following technique:\n",
    "\n",
    "One of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine K.\n",
    "\n",
    "A number of other techniques exist for validating K, but that's the most common one. \n",
    "\n",
    "In any case, back to our clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10) # default is also 10, but good to know \n",
    "\n",
    "km.fit(tfidf_vectorizer_vectors)\n",
    "\n",
    "# km.labels_ gives you the cluster assignments\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump our clusters into a dataframe\n",
    "films = { 'title': titles, 'idx': list(range(100)), 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "film_df = pd.DataFrame(films, columns = ['title', 'idx', 'cluster', 'genre'])\n",
    "\n",
    "film_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many films are in each cluster\n",
    "film_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top terms per cluster\n",
    "\n",
    "# this orders by the distance of each term from the center\n",
    "# (cluster_centers_ returns an array of [n_clusters, n_features] )\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster \" + str(i) + \" top words: \")\n",
    "    top_terms = \"\"\n",
    "   \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        top_terms += terms[ind] + \", \"\n",
    "  \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    print(\"Titles in cluster \" + str(i) + \": \")\n",
    "    cluster_titles = \"\"\n",
    "\n",
    "    # create new df of only the specific cluster\n",
    "    # remember boolean selection! \n",
    "    cluster_df = film_df[ film_df[\"cluster\"] == i ]\n",
    " \n",
    "    # create series of titles assoc w/ that cluster \n",
    "    for title in cluster_df['title']: \n",
    "        cluster_titles += title + \", \"\n",
    "\n",
    "    print(cluster_titles + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 films in each cluster    \n",
    "for i in range(num_clusters):  \n",
    "    \n",
    "    # returns array w/ distances to the centroid i \n",
    "    dist = km.transform(tfidf_vectorizer_vectors)[:, i] \n",
    "    \n",
    "    # return indices for top 10 for that cluster \n",
    "    idxs = np.argsort(dist)[::][:10] \n",
    "    \n",
    "    print(\"Top 10 films in cluster \" + str(i) + \": \")\n",
    "    cluster_top_films = \"\"\n",
    "\n",
    "    # look up film title by index\n",
    "    for idx in idxs: \n",
    "        cluster_top_films += film_df.loc[idx,'title'] + \", \"\n",
    "        \n",
    "    print(cluster_top_films + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with T-SNE\n",
    "\n",
    "You have probably heard of **t-sne** (is it TEA SNEA? or TAE SNAE..)!  This is \"newer\" dimension reduction method that emphasizes visual convenience. Sometimes PCA (which we have used in the past) can produce overlapping/crowding of similar points. The con of tsne is that it is not as easily interpretable as PCA. It's also non-deterministic -- you'll get different but similar results everytime. But I thought you should play with it since it is widely used in machine learning today.\n",
    "\n",
    "Example projects: [hip hop](https://pudding.cool/2017/09/hip-hop-words/) and [wikipedia political ideologies](https://genekogan.com/works/wiki-tSNE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "embed = tsne.fit_transform(tfidf_vectorizer_vectors.toarray())\n",
    "xs, ys = zip(*embed) # we'll use these coords below, after we set up our plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing document clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Cluster 0', \n",
    "                 1: 'Cluster 1', \n",
    "                 2: 'Cluster 2', \n",
    "                 3: 'Cluster 3', \n",
    "                 4: 'Cluster 4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the t-sne plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) # here's where the coords come in \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# iterate through groups to layer the plot\n",
    "# note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return \n",
    "# the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "    \n",
    "plt.show() #show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of cosine similarity with respect to document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is measured against the tf-idf vectors and can be used to generate a measure of \n",
    "similarity between each document and the other documents in the corpus (each synopsis among the synopses). \n",
    "\n",
    "We have seen this before in word2vec's similarity method, but here it is again. For example, here is how you would find the most similar film (in terms of td-idf of synopses) to the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similar_to_0 = cosine_similarity(tfidf_vectorizer_vectors[0:1], tfidf_vectorizer_vectors)\n",
    "\n",
    "# return indices sorted by cosine similarity \n",
    "idxs = np.argsort(-similar_to_0)[::]\n",
    "\n",
    "print(\"Most similar films to \" + film_df.loc[0,'title'] + \":\")\n",
    "       \n",
    "# look up film title by index -- only look at top 10\n",
    "for idx in idxs[0][:10]: \n",
    "    print(film_df.iloc[idx]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
