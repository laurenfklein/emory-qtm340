{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty_rRD1oMqtG"
   },
   "source": [
    "# Extra Bits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few extra things we can do with classifiers\n",
    "\n",
    "We'll need to retrain our classifer from class 15..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jyp21rXfMqtO"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLNevA0OMqvh",
    "outputId": "2f67e4a8-352c-415c-c3fc-baa30e727169"
   },
   "outputs": [],
   "source": [
    "# back to the newsgroup data; load the categories we want and create our test and training data\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.autos', 'rec.motorcycles',\n",
    "              'sci.space', 'comp.graphics']\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories) # quickly make a train dataset\n",
    "test = fetch_20newsgroups(subset='test', categories=categories) # quickly make a test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3U72sYEMqw3"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab_type": "text",
    "id": "vyFspFW3MqxB"
   },
   "outputs": [],
   "source": [
    "# predict labels for our test data\n",
    "\n",
    "X_new_counts = count_vect.transform(test.data)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "labels = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  comp.graphics       0.97      0.92      0.94       389\n",
      "      rec.autos       0.92      0.96      0.94       396\n",
      "rec.motorcycles       0.96      0.96      0.96       398\n",
      "      sci.space       0.96      0.97      0.96       394\n",
      "\n",
      "       accuracy                           0.95      1577\n",
      "      macro avg       0.95      0.95      0.95      1577\n",
      "   weighted avg       0.95      0.95      0.95      1577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just check some metrics before we move on\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(test.target, labels, target_names=train.target_names)\n",
    "\n",
    "print(report) # f1-score is harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraxting most informative features\n",
    "\n",
    "Here's one new thing: we can extract the most informative features for each label. This uses the `coef_` attribute, which interprets `MultinomialNB()` as a linear model, giving the empirical log probability of each feature given a class, P(x_i|y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for comp.graphics (label 0): \n",
      "3d, organization, com, subject, files, lines, university, image, edu, graphics, \n",
      "Top features for rec.autos (label 1): \n",
      "just, lines, organization, subject, writes, article, cars, com, edu, car, \n",
      "Top features for rec.motorcycles (label 2): \n",
      "lines, subject, organization, writes, ca, article, dod, edu, bike, com, \n",
      "Top features for sci.space (label 3): \n",
      "toronto, com, gov, moon, alaska, access, henry, nasa, edu, space, \n"
     ]
    }
   ],
   "source": [
    "# extract most informative features -- NOTE TO LK, FIGURE OUT COEF VAR\n",
    "\n",
    "def print_top10(vect, clf, labels):\n",
    "    feature_names = vect.get_feature_names()\n",
    "    for i, label in enumerate(train.target_names):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"Top features for \" + label + \" (label \" + str(i) + \"): \") \n",
    "        top_features = \"\"\n",
    "        for j in top10:\n",
    "            top_features += str(feature_names[j]) + \", \"\n",
    "        print(top_features)\n",
    "        \n",
    "print_top10(count_vect, clf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probablity for all classes given a test vector X\n",
    "\n",
    "def predict_probs(s, model=clf):\n",
    "    test_str = [s] # need to turn the string into an array first\n",
    "    X_new_counts = count_vect.transform(test_str)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    \n",
    "    pred = model.predict_proba(X_new_tfidf)\n",
    "    for i, label in enumerate(train.target_names):\n",
    "        print(\"Probability of being classified as \" + label + \" (label \" + str(i) + \"): \" + str(pred[0][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being classified as comp.graphics (label 0): 0.20380777835091615\n",
      "Probability of being classified as rec.autos (label 1): 0.19778891182239855\n",
      "Probability of being classified as rec.motorcycles (label 2): 0.18993853422928691\n",
      "Probability of being classified as sci.space (label 3): 0.4084647755973983\n"
     ]
    }
   ],
   "source": [
    "predict_probs(\"sending a payload to the ISS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this is super easy to do with [TextBlob](https://textblob.readthedocs.io/en/dev/classifiers.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some basic training data (note that you can also load in data from files)\n",
    "\n",
    "train = [\n",
    "     ('I love this sandwich.', 'yum'),\n",
    "     ('this is an amazing sundae!', 'yum'),\n",
    "     ('I feel very good about these dumplings.', 'yum,'),\n",
    "     ('this is the most delicious salad I have ever had.', 'yum'),\n",
    "     (\"what a delicious pizza\", 'yum'),\n",
    "     ('I do not like this restaurant', 'yuck'),\n",
    "     ('I am tired of this chicken.', 'yuck'),\n",
    "     (\"I can't deal with all this mayonnaise\", 'yuck'),\n",
    "     ('mayonnaise is my sworn enemy!', 'yuck'),\n",
    "     ('my sandwich is horrible.', 'yuck')\n",
    "]\n",
    "\n",
    "test = [\n",
    "     ('the sandwich was good.', 'yum'),\n",
    "     ('I do not enjoy mayonnaise', 'yum'),\n",
    "     (\"The special salad was not very good today.\", 'yuck'),\n",
    "     (\"This tastes amazing!\", 'yum'),\n",
    "     ('I always like to put sprinkles on my ice cream come.', 'yum'),\n",
    "     (\"I can't believe I'm eating this.\", 'yuck')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we’ll create a Naive Bayes classifier, passing the training data into the constructor.\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "clf = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yum'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classify(\"This is an amazing hot dog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the label probability distribution with the prob_classify(text) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yum'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist = clf.prob_classify(\"This one's delicious.\")\n",
    "\n",
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the label probability distribution with the prob_classify(text) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"yum\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"yuck\"), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying TextBlobs\n",
    "Another way to classify text is to pass a classifier into the constructor of TextBlob and call its classify() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yum'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The pasta is delicious. But the sauce is horrible.\", classifier=clf)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this approach is that you can classify sentences within a TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pasta is delicious.\n",
      "yum\n",
      "But the sauce is horrible.\n",
      "yuck\n"
     ]
    }
   ],
   "source": [
    "for s in blob.sentences:\n",
    "    print(s)\n",
    "    print(s.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Classifiers\n",
    "To compute the accuracy on our test set, use the accuracy(test_data) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the show_informative_features() method to display a listing of the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(this) = False            yum, : yum    =      2.5 : 1.0\n",
      "     contains(delicious) = False            yuck : yum    =      1.8 : 1.0\n",
      "            contains(my) = False             yum : yuck   =      1.5 : 1.0\n",
      "    contains(mayonnaise) = False             yum : yuck   =      1.5 : 1.0\n",
      "            contains(is) = False            yum, : yum    =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show informative features\n",
    "\n",
    "clf.show_informative_features(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractors\n",
    "\n",
    "By default, the NaiveBayesClassifier uses a simple feature extractor that indicates which words in the training set are contained in a document.\n",
    "\n",
    "For example, the sentence “I love ice cream sundaes” might have the features contains(snndaes): True or contains(cones): False.\n",
    "\n",
    "You can override this feature extractor by writing your own. A feature extractor is simply a function with document (the text to extract features from) as the first argument. The function may include a second argument, train_set (the training dataset), if necessary.\n",
    "\n",
    "The function should return a dictionary of features for document.\n",
    "\n",
    "For example, let’s create a feature extractor that just uses the first and last words of a document as its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_word_extractor(document):\n",
    "    tokens = document.split()\n",
    "    first_word, last_word = tokens[0], tokens[-1]\n",
    "    feats = {}\n",
    "    feats[\"first({0})\".format(first_word)] = True\n",
    "    feats[\"last({0})\".format(last_word)] = False\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the feature extractor in a classifier by passing it as the second argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yum'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = NaiveBayesClassifier(test, feature_extractor=end_word_extractor)\n",
    "blob = TextBlob(\"Ice cream sundaes are my very favorite!\", classifier=clf2)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note about how this can be used to filter *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "05.05-Naive-Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
